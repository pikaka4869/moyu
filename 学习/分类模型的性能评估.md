## 分类模型的性能评估

### 混淆矩阵（Confusion Matrix）
混淆矩阵是评估分类模型性能的基础工具，它展示了模型预测结果与实际标签的匹配情况。对于二分类问题，混淆矩阵包含以下四个关键指标：
- **真正例（True Positive, TP）**：实际为正类且被正确预测为正类的样本数
- **假正例（False Positive, FP）**：实际为负类但被错误预测为正类的样本数
- **真负例（True Negative, TN）**：实际为负类且被正确预测为负类的样本数
- **假负例（False Negative, FN）**：实际为正类但被错误预测为负类的样本数

### 召回率（Recall）/ 真正率（True Positive Rate, TPR）
召回率也称为敏感度（Sensitivity）或真正率（True Positive Rate, TPR），表示实际正类样本中被模型正确识别的比例。
公式：
```
Recall = TPR = TP / (TP + FN)
```
意义：衡量模型对正类样本的捕捉能力，适用于关注漏检率的场景（如疾病诊断）。

### 精确率（Precision）
精确率表示模型预测为正类的样本中实际为正类的比例。
公式：
```
Precision = TP / (TP + FP)
```
意义：衡量模型预测结果的准确性，适用于关注误判成本高的场景（如垃圾邮件检测）。

### F1值（F1 Score）
F1值是精确率和召回率的调和平均数，综合考虑了两者的性能。
公式：
```
F1 Score = 2 * (Precision * Recall) / (Precision + Recall)
```
意义：当需要平衡精确率和召回率时使用，尤其适用于类别不平衡的数据集。

### 特异度（Specificity）
特异度表示实际负类样本中被模型正确识别的比例。
公式：
```
Specificity = TN / (TN + FP)
```
意义：衡量模型对负类样本的识别能力。

### 假正值（False Positive Rate, FPR）
假正值表示实际负类样本中被模型错误预测为正类的比例。
公式：
```
FPR = FP / (TN + FP) = 1 - Specificity
```
意义：衡量模型的误判率。



### ROC 曲线和 AUC（Receiver Operating Characteristic Curve & Area Under Curve）
- **ROC曲线**：以FPR为横轴，TPR为纵轴绘制的曲线，反映不同阈值下模型的分类性能
- **AUC**：ROC曲线下的面积，取值范围为0-1
  - AUC=0.5：模型性能与随机猜测相当
  - AUC>0.5：模型性能优于随机猜测
  - AUC=1：模型完美分类
意义：ROC曲线和AUC适用于评估二分类模型在不同阈值下的整体性能，尤其适用于类别不平衡的数据集。

### 平衡精度（Balanced Accuracy）
平衡精度是召回率和特异度的平均值，用于处理类别不平衡的情况。
公式：
```
Balanced Accuracy = (TPR + Specificity) / 2
```
意义：在类别不平衡时，比传统准确率更能反映模型的真实性能。

### Matthews 相关系数（MCC）
Matthews相关系数综合考虑了混淆矩阵的所有四个指标，取值范围为-1到1。
公式：
```
MCC = (TP * TN - FP * FN) / sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))
```
意义：
- MCC=1：完全正确的预测
- MCC=0：预测结果与随机猜测相当
- MCC=-1：完全错误的预测
适用于类别不平衡的数据集，提供比F1值更全面的性能评估。
